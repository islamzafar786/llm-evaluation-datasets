[
    {
        "title": "Generative Adversarial Networks for Multi-Instrument Music Synthesis",
        "university": "University of Passau",
        "year": "2020",
        "abstract": "Generative Adversarial Networks (GANs) recently succeeded in various tasks of humans creative domain. In this masters thesis, based on a GAN a score-to-audio model is designed, built and analyzed. The model termed orGAN interprets sheet music by controlling on- and offsets, volume, instrumental interaction etc., taking the role of a human performer. It is capable of synthesizing scores of arbitrary length for arbitrary combinations of 13 types of instruments at once with a very high-quality sampling rate of 48kHz. For this, scores represented as pianorolls are transformed into a spectrogram. This way, generative model architectures from computer vision become applicable. After investigating the methodological concepts in depth, this masters thesis adapts a well-known image-to-image translation neural network architecture: orGAN is a conditional PatchGAN with fully-convolutional U-Net generator and an auxiliary instrument classifier. Experiments confirm the findings of pix2pix [27] regarding the patch size and show the superiority of the Multi-Scale Structural Similarity Index (MS-SSIM) over the L1 loss in a composite objective function. The instrument classifier is found to foster transfer learning from single- to multi-instrument play. Extensive human evaluation shows a superiority of orGAN over related work and state-of-the art synthesizers in naturalness, timbre and emotional expressiveness of the generated audio.",
        "contents": "GAN architecture, training datasets, evaluation metrics, application to music generation.",
        "problem_description": "Music synthesis has traditionally relied on rule-based systems and digital signal processing techniques. However, these approaches struggle to generate complex multi-instrument compositions that sound natural. With recent advancements in machine learning, particularly Generative Adversarial Networks (GANs), there is an opportunity to create more expressive and high-quality synthesized music that can closely resemble human performances.",
        "problem_motivation": "This thesis focuses on using GANs to synthesize multi-instrumental music by transforming sheet music representations into realistic audio outputs. The proposed model, orGAN, employs a PatchGAN-based architecture combined with a U-Net generator and an auxiliary instrument classifier to improve sound quality. The study evaluates the model using human perception tests, confirming that it outperforms conventional synthesis techniques in expressiveness, timbre, and naturalness.",
        "Topic":"Generative AI"
      },
      {
        "title": "Machine Learning Image Segmentation to Improve Object Recognition in Mixed Reality",
        "university": "Technical University of Munich (TUM)",
        "year": "2020",
        "abstract": "Object recognition represents an emerging technology in the field of image processing able to detect and label objects through the recognition of patterns in images. At the same time, Mixed Reality represents the combination of the virtual and physical worlds in a bid to yield a digital environment where elements from both dimensions co-exist. Through the integration of an image segmentation algorithm along with image enhancement techniques, this thesis aims to facilitate the navigation experience in Mixed Reality by recognizing more efficiently those objects that provide relevant information to users to navigate. The image segmentation algorithm and the image enhancement techniques are implemented in a video recording, in such a way that through the detection and modification of object features, their instances are either visually highlighted or downgraded according to the information they provide to fulfill the navigation task. Subsequently, in order to determine the impact on human perception, two user tests are conducted. In the first test, users are asked to focus their attention on a virtual element and select the objects that attract their attention the most. In the second test, in which the methodology of this thesis is implemented, users are also asked to focus on a virtual element added to the video and choose the elements that are most striking to them. The results show that the technique used to highlight objects allowed users to recognize them more easily. In contrast, the objects that were downgraded remained eye-catching to users.",
        "contents": "Image segmentation, mixed reality applications, convolutional neural networks (CNNs), dataset augmentation.",
        "problem_description": "Mixed Reality (MR) applications require robust object recognition to blend digital and real-world elements seamlessly. However, traditional object detection methods struggle with complex and dynamic environments, leading to inaccurate or delayed recognition. Improved image segmentation techniques can enhance object detection and user interactions in MR settings.",
        "problem_motivation": "This research integrates image segmentation algorithms with image enhancement techniques to improve object recognition in MR environments. The study conducts user experiments to evaluate the effectiveness of these enhancements in guiding attention and navigation. Results indicate that the proposed approach significantly improves object visibility and recognition efficiency in MR applications.",
        "Topic":"Computer Vision"
      },
      {
        "title": "Self-supervised Domain Adaptation of Language Models for the Process Industry",
        "year": "2024",
        "abstract": "Incorporating additional knowledge into pre-trained language models (PLMs) has proven to be highly effective in improving their performance in specialized fields. Graph structures, in particular, allow models to capture domain-specific relationships between documents that would otherwise be missed. In the process industry, where unstructured text logs document critical operational insights, leveraging these relationships becomes essential for improving document representation. This thesis proposes a graph-aware domain adaptation method aimed at enhancing the representations of PLMs for the process industry. Building upon SciNCL, a neighborhood contrastive learning approach, the study constructs a process industry graph comprising functional locations and maintenance text logs to sample document pairs for contrastive learning. The proposed method outperformed baseline models in a semantic search task, with the best-performing model achieving an nDCG@10 score 9 points higher than the best baseline. These findings encourage further exploration of graph-based domain adaptation techniques, particularly in domains characterized by sparse document connections and limited data availability.",
        "contents": "Self-supervised learning, language models, domain adaptation, process industry applications.",
        "problem_description": "This thesis presents a graph-aware self-supervised learning approach to adapt language models to the process industry. By constructing a process industry graph and leveraging contrastive learning, the model enhances document representation. The study demonstrates that the proposed method improves semantic search performance and encourages further research into graph-based domain adaptation techniques.",
        "problem_motivation": "Pre-trained language models perform well in general domains but struggle with specialized industrial contexts due to the lack of labeled data. The process industry relies heavily on unstructured text logs, and improving language model adaptation can enhance information retrieval, fault detection, and predictive maintenance.",
        "Topic":"Natural Language Processing"
      },
      {
        "title": "Deep Learning Techniques Applied to Constituency Parsing of German",
        "university": "University of Koblenz Landau",
        "year": "2020",
        "abstract": "",
        "problem_description": "This study develops a neural network-based constituency parser for German. The approach leverages self-attention mechanisms and multi-layered architectures to improve syntactic structure extraction. The model achieves state-of-the-art accuracy on benchmark datasets, demonstrating the effectiveness of deep learning for German constituency parsing.",
        "problem_motivation": "Natural language processing applications such as grammar checking and machine translation require accurate syntactic parsing. Traditional parsing methods for German have limitations due to the language's complex word order and inflectional morphology. Deep learning techniques offer a promising solution to improve parsing accuracy.",
        "Topic":"Natural Language Processing"
      },
      {
        "title": "Applying Deep Reinforcement Learning in the Navigation of Mobile Robots in Static and Dynamic Environments",
        "year": "2019",
        "abstract": "Nowadays mobile robots operate reliably in clean static environments like industry setups, but rather fail in complex dynamic environments, like e.g. airports or shopping centers. Those environments are more crowded and narrow than industry-like environments and contain moving objects — mainly humans. Traditional navigation approaches treat all objects as static objects, resulting in a non-reasonable behavior. Mobile robots should be able to cope with dynamic objects as well as dynamic crowds. In this thesis, local planning is realized with the state-of-the-art Deep Reinforcement Learning (DRL) approach Proximal Policy Optimization (PPO). The RL-agent is trained in a 2D-simulation environment, where it collects experiences to update the Deep Neural Network, that serves as a function approximator. First, several RL-agents are trained in a static industry-like task setup and compared to traditional navigation approaches. Second, profiting from the knowledge of the static training, agents were trained in adynamic environment with simulated humans, behaving according to Helbing’s Social Force Model. Two different behaviors worth mentioning have been evolved. One agent learned a policy, that avoids individual humans, but stops and waits if the robot faces unsolvable situations like crowds or blocked passages. The other agent learned a more aggressive policy. It can push pedestrians by driving very slowly towards them until they give way.",
        "contents": "Reinforcement learning, mobile robotics, navigation, deep Q-networks, policy optimization.",
        "university": "Universität Hamburg",
        "problem_description": "This thesis employs Proximal Policy Optimization (PPO) to train reinforcement learning agents for robot navigation. The agents are trained in simulated environments, learning policies to handle human crowds and obstacles. The study compares different training strategies and evaluates their effectiveness in improving autonomous navigation.",
        "problem_motivation": "Autonomous mobile robots must navigate environments that contain both static and dynamic obstacles. Traditional navigation approaches assume static obstacles, leading to inefficient or unsafe behavior in dynamic settings such as airports and shopping centers. Reinforcement learning can help robots adapt to dynamic environments.",
        "Topic":"Deep Learning"

      },
      {
        "title": "Graph Neural Networks for Electrical Grid State Estimation",
        "year": "2024",
        "abstract": "Increasing loads on the Low-voltage network (LVN) require constant status checks.The health status of a LVN can be determined using an automated Electrical grid power system state estimation (SE). The reformulation of the State estimation task (SE-task) uses heterogeneous graph concepts. Using Graph Neural Networks (GNNs), applied complex voltage can be gained in spare measured girds. A date-fitting concept is presented, for parsing LVN into a proper data structure. The author presents the GNN GSETR, capable of performing the SE. A great performance of 99 % is reached by GSETR, knowing all power measurements. Moreover, this work demonstrates the robustness of the model. For this purpose, the measured power data is only sparsely available to the model. A 91 % accuracy is achieved, in this second scenario.",
        "contents": "Graph neural networks, electrical grid optimization, power system state estimation.",
        "problem_description": "This research introduces GSETR, a GNN-based model for electrical grid state estimation. The model uses graph structures to infer voltage states in low-voltage networks with sparse measurements. Experimental results demonstrate that the proposed method achieves high accuracy, even with limited sensor data.",
        "problem_motivation": "Modern electrical grids require real-time state estimation to maintain stability and efficiency. Traditional estimation methods struggle with incomplete data and computational constraints. Graph Neural Networks (GNNs) offer a promising approach to enhance state estimation accuracy in power systems.",
        "Topic":"Neural Networks"
      },
      {
        "title": "Representation Learning on Electronic Health Records Using Graph Neural Networks",
        "year": "2023",
        "abstract": "The digitization of healthcare has led to a proliferation of electronic health records, providing valuable data for machine learning algorithms(e.g., graph neural networks) to make accurate predictions about patients outcomes(e.g., mortality prediction). This thesis investigates different aspects of representation learning on electronic health records using a graph neural network and provides new insights into the area of graph modelling, feature ablation, and helps to understand the effect of bias in the graph structure. It also evaluates the models underlying predictors with well-established statistical models (SAPS-II & SAPS-III) for predicting the mortality of the patients diagnosed with sepsis using the MIMIC-III dataset. The experimentation shows that the lab and vital signs features used in predicting mortality in SAPS-II and SAPS-III are ranked in the top 90 percentile amongst the predictors of mortality in the used heterogeneous Graph Attention Network (GAT) model. Experimentation with different graph representations(different ways of representing data in nodes and edges in a heterogeneous graph) shows their advantages and disadvantages. However, in terms of area under receiver operating characteristics curve (Area under the receiver operating curves (AUROC)), different representations performed similarly well. The general way of modelling time-dependent measurements with multiple edges without any aggregation or transformation of edge data had no bias but performed worse in GPU utilization and memory usage. Different ways of encoding the categorical and text data also had an impact on the models performance, wherein the encoding of such data with a clinical text pre-trained UMLSBert model had better performance than the label or one-hot encoding. Furthermore, the GAT model is tested by introducing an additionally highly biased relationship (similar demography). It was seen that the models attention mechanism corrected such a nature of bias. Finally, the experiments showed that drugs were the best predictors of mortality among labs, vitals, or diagnoses.",
        "contents": "Graph neural networks, healthcare analytics, representation learning, patient record analysis.",
        "problem_description": "This thesis explores the application of GNNs to EHR data for mortality prediction. The study evaluates different graph representations and encoding techniques, demonstrating that GNN-based models outperform traditional methods in predicting patient outcomes. The findings highlight the potential of GNNs in medical analytics.",
        "problem_motivation": "Electronic health records (EHRs) contain valuable patient data that can improve medical decision-making. However, existing predictive models struggle with complex relationships in patient data, leading to suboptimal performance. Graph Neural Networks (GNNs) can enhance representation learning and improve patient outcome predictions.",
        "Topic":"Data Analytsis"
      },
      {
        "title": "Deep Reinforcement Learning for Decentralized Autonomous Decision-Making in Federated Satellite Systems",
        "year": "2024",
        "abstract": "Constituent parsing attempts to extract syntactic structure from a sentence. These parsing systems are helpful in many NLP applications such as grammar checking, question answering, and information extraction. This thesis work is about implementing a constituent parser for German language using neural networks. Over the past, recurrent neural networks have been used in building a parser and also many NLP applications. In this, self-attention neural network modules are used intensively to understand sentences effectively. With multi-layered self-attention networks, constituent parsing achieves 93.68% F1 score. This is improved even further by using both character and word embeddings as a representation of the input. An F1 score of 94.10% was the best achieved by constituent parser using only the dataset provided. With the help of external datasets such as German Wikipedia, pre-trained ELMo models are used along with self-attention networks achieving 95.87% F1 score.",
        "contents": "Federated learning, reinforcement learning, autonomous satellites, decentralized control.",
        "problem_description": "This study applies deep reinforcement learning (DRL) to federated satellite networks, training agents to optimize observation, data sharing, and energy management tasks. The research compares different DRL algorithms and evaluates their effectiveness in decentralized satellite coordination. The results indicate that DRL-based approaches can significantly improve mission efficiency.",
        "problem_motivation": "The growing number of satellites in orbit necessitates autonomous decision-making for efficient resource management. Centralized control is infeasible for large satellite constellations due to communication delays and scalability issues. Decentralized learning approaches can enable autonomous coordination among satellites.",
        "Topic":"Deep Learning"
      },
      {
        "title": "Solving Machine Learning Problems",
        "problem_description": "This thesis presents a machine learning model capable of solving university-level machine learning problems using a dataset generated from MITs 6.036 Introduction to Machine Learning course. The model is trained on a dataset consisting of course exercises, homework, and quiz questions, spanning 12 major machine learning topics. The system employs Transformer models within an encoder-decoder architecture enhanced with graph neural networks (GNNs) and expression trees to generate structured responses. By successfully training a machine learning model to answer machine learning questions at an expert level, this work establishes a foundation for AI-driven education in technical disciplines. The system demonstrates real-time problem-solving capabilities and opens new avenues for automated tutoring and assessment systems in machine learning education.",
        "problem_motivation": "Machine learning models have demonstrated remarkable success in various tasks, but their application to solving structured machine learning problems remains largely unexplored. While natural language processing models like GPT-3 can handle text-based reasoning, they struggle with STEM subjects that require formal logic and mathematical problem-solving. Current state-of-the-art models fail at solving university-level coursework, particularly in technical disciplines like machine learning. The challenge lies in teaching machines to understand and reason through structured problems in an educational setting. By developing an automated system that can learn machine learning concepts and solve related coursework, we can advance AI’s role in STEM education and create intelligent tutoring systems that assist students in their learning journey.",
        "Year": "2021",
        "Abstract": "Can a machine learn Machine Learning? This work trains a machine learning model to solve machine learning problems from a University undergraduate level course. We generate a new training set of questions and answers consisting of course exercises, homework, and quiz questions from MIT’s 6.036 Introduction to Machine Learning course and train a machine learning model to answer these questions. Our system demonstrates an overall accuracy of 96% for open-response questions and 97% for multiple-choice questions, compared with MIT students’ average of 93%, achieving grade A performance in the course, all in real-time. Questions cover all 12 topics taught in the course, excluding coding questions or questions with images. Topics include: (i) basic machine learning principles; (ii) perceptrons; (iii) feature extraction and selection; (iv) logistic regression; (v) regression; (vi) neural networks; (vii) advanced neural networks; (viii) convolutional neural networks; (ix) recurrent neural networks; (x) state machines and MDPs; (xi) reinforcement learning; and (xii) decision trees. Our system uses Transformer models within an encoder-decoder architecture with graph and tree representations. An important aspect of our approach is a data-augmentation scheme for generating new example problems. We also train a machine learning model to generate problem hints. Thus, our system automatically generates new questions across topics, answers both open-response questions and multiple-choice questions, classifies problems, and generates problem hints, pushing the envelope of AI for STEM education.",
        "Topic": "Machine Learning"
    },
    {
        "title": "Optimization Methods for Machine Learning underStructural Constraints",
        "problem_description":"This thesis develops scalable optimization methods for constrained machine learning problems by addressing various structural constraints. First, it proposes novel algorithms for subgradient regularized convex regression, ensuring linear convergence and efficiently solving large-scale instances with high-dimensional data. Next, it introduces a computational framework for log-concave density MLE, leveraging smoothing techniques and integral discretization to achieve significant runtime improvements over existing convex approaches. Additionally, the thesis presents an ℓ₀ℓ₂-penalized pseudolikelihood estimator for Gaussian Graphical Models, demonstrating superior computational efficiency and statistical performance compared to traditional ℓ₁-based methods. Finally, it enhances Branch-and-Bound (BnB) solvers for sparse learning by incorporating a screening procedure that reduces computational overhead, enabling faster optimization in high-dimensional problems. These advancements contribute to more efficient and scalable solutions for complex machine learning models.",
        "problem_motivation":"Modern statistical and machine learning models impose structural constraints for interpretability and complexity reduction. Efficient optimization methods are needed to handle large-scale problems with constraints like shape restrictions in nonparametric statistics and sparsity in high-dimensional statistics.",
        "Year": "2022",
        "Abstract": "In this thesis, we present scalable optimization methods for large-scale machine learning problems under structural constraints, focusing on shape constraints in nonparametric statistics and sparsity in high-dimensional statistics.We first address the subgradient regularized convex regression problem, proposing novel large-scale algorithms based on proximal gradient descent and active set methods, with linear convergence guarantees. Our framework efficiently solves instances with n=105 n=105, d=10 d=10 within minutes. Next, we develop a new computational framework for log-concave density MLE, utilizing smoothing techniques and integral discretization to improve runtime over earlier convex approaches. For Gaussian Graphical Models, we propose an ℓ₀ℓ₂-penalized pseudolikelihood estimator, solving a mixed integer programming (MIP) formulation via a specialized nonlinear Branch-and-Bound (BnB) framework. Our method scales to v<=10,000 p<=10,000 and outperforms ℓ₁-based approaches in both speed and statistical performance.Finally, we enhance the BnB framework for sparse learning with a novel screening procedure that fixes relaxed variables to 0 or 1, significantly reducing solver runtimes.",
        "Topic":"Machine Learning"
    },
    {
        "title":"Probabilistic data analysis with probabilistic programming",
        "problem_description":"This thesis defines Composable Generative Population Models (CGPMs) as a structured way to describe probabilistic models and integrate them into BayesDB for performing probabilistic data analysis. CGPMs specify a table of observable random variables, enabling complex intra-row dependencies as well as inter-row relationships mediated through latent variables. The framework supports multiple probabilistic modeling techniques, including hierarchical Bayesian models, kernel density estimation, discriminative machine learning, clustering algorithms, and dimensionality reduction methods.",
        "problem_motivation":"Probabilistic techniques play a crucial role in data analysis but are often difficult to apply, combine, and compare across different modeling approaches. Traditional methods such as parametric statistical modeling, machine learning, and probabilistic programming each have their own formalisms and assumptions, making it challenging to create a unified framework for probabilistic data analysis. To address these challenges, this thesis introduces Composable Generative Population Models (CGPMs)—a novel computational abstraction that extends graphical models for use in probabilistic programming. By integrating CGPMs into BayesDB, a probabilistic programming platform, this research aims to provide a more flexible and scalable approach to probabilistic data analysis across a variety of tasks.",
        "Year": "2016",
        "Abstract": "Probabilistic techniques are central to data analysis, but dierent approaches can be challenging to apply, combine, and compare. This thesis introduces composable generative population models (CGPMs), a computational abstraction that extends directed graphical models and can be used to describe and compose a broad class of probabilistic data analysis techniques. Examples include hierarchical Bayesian models, multivariate kernel methods, discriminative machine learning, clustering algorithms, dimensionality reduction, and arbitrary probabilistic programs. We also demonstrate the integration of CGPMs into BayesDB, a probabilistic programming platform that can express data analysis tasks using a modeling language and a structured query language. The practical value is illustrated in two ways. First, CGPMs are used in an analysis that identifies satellite data records which probably violate Kepler's Third Law, by composing causal probabilistic programs with non-parametric Bayes in under 50 lines of probabilistic code. Second, for several representative data analysis tasks, we report on lines of code and accuracy measurements of various CGPMs, plus comparisons with standard baseline solutions from Python and MATLAB libraries.",
        "Topic":"Data Analysis"
    },
    {
        "title":"Artificial intelligence-assisted data analysis with BayesDB",
        "problem_description":"This thesis addresses challenges in real-world data analysis, particularly missing data and predictive model errors, using BayesDB, a probabilistic programming platform. Traditional imputation methods often fail to capture dependencies in data, leading to biased or inaccurate results. This work leverages BayesDB CrossCat model to improve missing data imputation and analyze predictive errors. First, it explores detecting systematic missing data patterns, distinguishing between MCAR, MAR, and MNAR scenarios to guide imputation strategies. Next, it evaluates CrossCat’s performance against standard imputation methods using the ANES dataset, demonstrating superior calibration and accuracy. Finally, it introduces a generative monitoring model to detect systematic errors in predictive models, analyzing bias using data from Gapminder and ANES. The findings highlight BayesDB's ability to enhance model interpretability and improve decision-making in data-driven applications.",
        "problem_motivation":"When applying machine learning and statistical techniques to real-world datasets, several challenges arise, particularly due to missing data and errors from predictive models. Traditional data analysis methods struggle to handle incomplete datasets effectively, leading to biased or inaccurate conclusions. Additionally, black-box predictive models often produce results that are difficult to interpret, making it challenging to assess their reliability. To address these issues, this thesis explores the use of BayesDB, a probabilistic programming platform that enables efficient modeling, imputation of missing values, and characterization of predictive errors. By leveraging BayesDB's CrossCat model, this research aims to enhance data analysis workflows, improve imputation accuracy, and provide better insights into the behavior of predictive models.",
        "Year": "2017",
        "Abstract": "When applying machine learning and statistics techniques to real-world datasets, problems often arise due to missing data or errors from black-box predictive models that are difficult to understand or explain in terms of the model's inputs. This thesis explores the applicability of BayesDB, a probabilistic programming platform for data analysis, to three common problems in data analysis: (i) modeling patterns of missing data, (ii) imputing missing values in datasets, and (iii) characterizing the error behavior of predictive models. Experiments show that CrossCat, the default model discovery mechanism used by BayesDB, can address all three problems effectively. Examples are drawn from the American National Election Studies and the Gapminder database of global macroeconomic and public health indicators.",
        "Topic":"Generative AI"
    },
    {
        "title":"Data analysis and simulation approach to capacity planning",
        "problem_description":"This study focuses on capacity planning for military mental health services, specifically at an outpatient unit in a U.S. Army installation (Site Alpha). It examines system inefficiencies, including provider shortages, patient distribution, and high-utilizer cases, which contribute to long wait times and unmet care demands. Using causal loop diagrams and simulation modeling techniques, the research evaluates how different resource allocations impact system performance. The results suggest that before adding more resources, inefficiencies in the existing system must be addressed, particularly in the distribution of providers and workload balancing. This thesis highlights the need for data-driven decision-making in capacity planning to ensure sustainable and effective mental health service delivery within military settings.",
        "problem_motivation":"The demand for mental health services within the U.S. military has increased significantly, particularly due to the long-term impact of deployments in recent conflicts. However, inefficiencies in the system prevent effective capacity planning, leading to bottlenecks, inadequate provider distribution, and delayed access to care. Traditional approaches to capacity expansion often fail due to systemic inefficiencies, making it necessary to adopt a holistic, systems-based approach to redesign mental health service capacity. This thesis seeks to improve the military mental health system by analyzing capacity constraints and proposing simulation-based models to optimize resource allocation.",
        "Year": "2015",
        "Abstract": "In 2012, President Obama signed an Executive Order to improve access to mental health service for active duty members and for veterans. Two years later, in 2014, the President outlined 19 new executive actions to improve the lives of service members with a focus on improving access to mental health care. These actions placed a priority on improving the capacity to provide mental health care. This thesis examines ways of improving the capacity of the mental health system with a focus on system redesign. I review capacity planning, provide a literature review of simulation methods and present a simulation, and data analysis of Site Alpha, a U.S. Army Installation. I also use causal loop diagrams to explore other feasible scenarios that affect care capacity. The key take-away from this work is that system inefficiencies should be dealt with before more resources can be effectively added and used in the system. Another pertinent finding is that the distribution of the providers in the system should be improved. The system also contains high utilizer patients who must be considered when planning for care. The mental health system is extremely complex and risks becoming even more complex. However, by adopting a holistic, systems approach to capacity planning the complexity can be managed.",
        "Topic":"Data Analysis"
    },
    {
        "title":"Faster linear algebra for data analysis and machine learning",
        "problem_description":"This thesis develops new fast linear algebra algorithms for large-scale machine learning and data analysis. It introduces importance sampling techniques to accelerate matrix approximation, SVD, and ridge regression, improving computational efficiency while preserving accuracy. The study also presents fast iterative methods for kernel-based learning, enhancing kernel PCA, kernel ridge regression, and kernel k-means clustering. Additionally, it explores randomized sketching techniques that drastically reduce memory and computational costs without sacrificing performance. The proposed methods are validated through experiments demonstrating their practical effectiveness in large-scale data analysis tasks.",
        "problem_motivation":"Linear algebra is fundamental to data analysis and machine learning, powering essential techniques like singular value decomposition (SVD), low-rank approximation, clustering, and kernel methods. However, as datasets grow, traditional matrix algorithms become computationally infeasible. This thesis addresses the need for faster, scalable, and memory-efficient linear algebra methods by leveraging random sampling, iterative refinement, and matrix sketching. These techniques aim to significantly reduce computation while maintaining provable accuracy, making large-scale machine learning applications more efficient.",
        "Year": "2018",
        "Abstract": "We study fast algorithms for linear algebraic problems that are ubiquitous in data analysis and machine learning. Examples include singular value decomposition and low-rank approximation, several varieties of linear regression, data clustering, and nonlinear kernel methods. To scale these problems to massive datasets, we design new algorithms based on random sampling and iterative refinement, tools that have become an essential part of modern computational linear algebra. We focus on methods that are provably accurate and efficient, while working well in practical applications. Open source code for many of the methods discussed in this thesis can be found at https://github.com/cpmusco.",
        "Topic":"Machine Learning"
    },
    {
        "title":"Emotional response modeling in financial markets : Boston Stock Exchange data analysis",
        "problem_description":"This study examines physiological responses (such as heart rate variability, skin conductance, and tension) in professional traders to identify their reactions to market events. Data was collected from traders at the Boston Stock Exchange, tracking physiological markers alongside financial transactions. The study defines key event markers, including trade size, bid/ask spread width, long vs. short positions, and profit/loss performance, to analyze how different trading conditions affect physiological states. The results indicate that negative performance events (e.g., losses) trigger stronger physiological responses than positive ones, with prolonged negative performance leading to greater stress indicators. This research highlights the critical role of emotional and physiological factors in financial risk processing, suggesting that behavioral insights can enhance trading strategies and decision-making models.",
        "problem_motivation":"Financial decision-making has traditionally been modeled using rational economic theories, assuming that market participants act logically based on all available information. However, behavioral finance suggests that emotional and physiological factors significantly influence risk perception and trading behavior. This thesis investigates the relationship between physiological responses and financial market decisions, aiming to understand how emotional reactions impact professional traders decision-making processes. By analyzing biometric data from market makers at the Boston Stock Exchange, the study seeks to uncover patterns of physiological changes associated with different trading events.",
        "Year": "2004",
        "Abstract": "In this thesis, physiological data is analyzed in the context of financial risk processing, specifically investigating the effects of financial trading decisions and situations on the physiological responses of professional market makers. The data for this analysis comes from an experiment performed on market makers at the Boston Stock Exchange. This analysis involved significant preprocessing of large financial and physiological data sets. Short-term and long term analysis of financial and performance based event markers of the data are performed and the results interpreted. There are two main conclusions. First, negative performance events are found to be the the main driver of physiological responses; positive performance events have minimal deviations from baseline physiological signals. Second, a long term analysis of events yield more substantial physiological changes than a short term analysis.",
        "Topic":"Data Analysis"
    },
    {
        "title": "Reverse Question Answering: Can an LLM Write a Question so Hard (or Bad) that it Can't Answer?",
        "problem_description":"This study investigates Reverse Question Answering (RQA), where an LLM generates a question given an answer and then attempts to answer it. It analyzes how well LLMs handle their own generated questions and assesses their reasoning consistency.",
        "problem_motivation":"While LLMs are widely used for question answering (QA), their ability to generate valid and well-formed questions remains unclear. Understanding where LLMs struggle with self-generated questions can help improve question-answering benchmarks and enhance reasoning reliability.",
        "Year": "2024",
        "Abstract": "Question answering (QA)—giving correct answers to questions—is a popular task, but we test reverse question answering (RQA): for an input answer, give a question with that answer. Past work tests QA and RQA separately, but we test them jointly, comparing their difficulty, aiding benchmark design, and checking reasoning consistency. We run 16 LLMs on QA and RQA with trivia questions/answers, revealing: 1) Versus QA, LLMs are much less accurate in RQA for numerical answers, but slightly more accurate in RQA for textual answers; 2) LLMs often answer their own invalid questions from RQA accurately in QA, so RQA errors are not from knowledge gaps alone; 3) RQA errors correlate with question difficulty and inversely correlate with answer frequencies in the Dolma corpus; and 4) LLMs struggle to provide valid multi-hop questions. By finding question and answer types that lead to RQA errors, we suggest improvements for LLM reasoning.",
        "Topic":"Natural Language Processing"
    },
    {
        "title": "Exploration of Different Large Language Models for Retrieval-Augmented Generation in Analyzing Wearable Running Data for Sports Physiotherapy",
        "problem_description":"The study investigates how different parameter-size Large Language Models (LLMs) perform in analyzing biomechanical running data from wearable devices using Retrieval-Augmented Generation (RAG). The goal is to determine the optimal model size that balances accuracy and computational efficiency for sports physiotherapy applications.",
        "problem_motivation":"Wearable devices generate biomechanical data that can be valuable for physiotherapists to prevent injuries and improve performance. However, processing this data effectively requires advanced AI models. Larger LLMs offer high accuracy but demand significant computational resources, making them impractical for some users. This research aims to identify the most efficient LLM size for real-world physiotherapy applications.",
        "Year": "2024",
        "Abstract": "This study investigated the impact of wearable technologies, particularly advanced biomechanical analytics and machine learning, on sports performance monitoring and intervention strategies within the realm of physiotherapy. The primary aims were to evaluate key performance metrics, individual athlete variations and the efficacy of machine learning-driven adaptive interventions.",
        "Topic":"Generative AI"
    },
    {
        "title": "Evaluating Large Language Models for Automated Cyber Security Alarm Analysis Processes",
        "problem_description":"The study investigates how Large Language Models (LLMs) can be used to automate the cybersecurity alarm analysis process, particularly in handling alert fatigue caused by an overwhelming number of security notifications. The research assesses different LLMs in their ability to analyze alarms, determine threat levels, and support security operators in decision-making.",
        "problem_motivation":"Cybersecurity professionals deal with a massive volume of security alerts, leading to 'alert fatigue,' where real threats might be overlooked due to an overload of information. Automating parts of this analysis using LLMs can enhance efficiency, reduce human workload, and improve response times in detecting genuine threats",
        "Year": "2024",
        "Abstract": "A security operations centre (SOC) is a facility where teams of security professionals, supported by advanced technologiesand processes, work together to monitor, detect, and respond to cybersecurity incidents. With advances in AI technology,most of the SOC functions are increasingly becoming AI-driven. Among these, real-time alert monitoring and triage is particularly important. Recent studies, by both industry and academia, have highlighted the problem of alert fatigue and burnout in SOC. Several solutions have been proposed in the literature and by the industry to address this problem. In this paper, we review the existing literature and industry solutions on alert fatigue mitigation through the lenses of automation, augmentation, and human-AI collaboration. Based on the review, we identify four major causes of alert fatigue in SOC. We also examine the shortcomings of existing solutions and propose several potential research directions leveraging AI. By providing a comprehensive analysis of the state-of-the-art approaches and their limitations, this study contributes to the existing literature in an important ield of study. We anticipate that it will inspire new research directions for addressing alert fatigue not just in SOCs but across other Command and Control (C2) domains as well.",
        "Topic":"Generative AI"
    },
    {
        "title": "Automatic Evaluation of Companies' Alignment with EU Taxonomy Using Large Language Models",
        "problem_description":"The research investigates how Large Language Models (LLMs) can automatically assess companies' alignment with the EU Taxonomy sustainability framework by analyzing corporate sustainability reports. It explores different prompting techniques and retrieval methods to optimize accuracy and relevance in extracting and evaluating information.",
        "problem_motivation":"Assessing compliance with the EU Taxonomy is currently a manual and time-consuming process for sustainability analysts. Automating this process with LLMs could improve efficiency, reduce human effort, and ensure consistent evaluation of sustainability reports.",
        "Year": "2024",
        "Abstract": "This project presents an end-to-end system using Large Language Models (LLM) and Retrieval-Augmented Generation (RAG) to automatically evaluate a company's EU Taxonomy performance based on their sustainability reports by answering two different questions: (1) What is the most suitable prompt between Zero-shot Chain-of-Thought (CoT), Few-shot CoT, and no-CoT prompting, and (2) What is the most suitable retriever system to retrieve EU Taxonomy-related information from company's reports. For the first question, we developed a qualitative human evaluation score to compare the answers' informativeness and correctness. We investigated whether automated metrics such as BERTScore or BLEU correlate with these human-evaluation scores. For the 2nd question, we compare different keyword extraction techniques (for keyword retrievers), query splitting and expansion techniques (for vector retrievers), and investigate the role of reranking in retriever systems. For question (1), results show that Zero-shot CoT prompting performs slightly better than traditional prompting followed by Few-shot CoT prompting, possibly due to the significantly longer prompts of Few-shot CoT. We also discovered that CoT prompting demonstrated a higher correlation between automated and human-evaluation metrics than noCoT prompting. Thus, it is easier to flag errors automatically. For the second question, we discovered: (i) Keyword extraction techniques do not concretely improve BM25 Keyword Retriever's performance; (ii) Splitting long queries into more self-contained sub-queries, whether using separators or using LLMs, achieves considerable performance boost for vector retriever; (iii) LLM-generated hypothetical answer also show significant improvement compared to the naive query splitting method; and (iv) Cross-Encoder reranking often filters out good results annotated by human, and the choice of reranking question also play a significant role in the Cross-Encoder Reranking model's performance. Finally, although our system and evaluation methods are not flawless, we have demonstrated that LLM and RAG can assist humans in extracting information related to EU taxonomy from a company's report and measuring that company's EU Taxonomy performance.",
        "Topic":"Generative AI"
    },
    {
      "title":"Variational Auto-Encoder for Latent Uncertainty Encoding in Large Language Models",
      "problem_description":"The research explores methods for encoding uncertainty in Large Language Models (LLMs) by introducing a Variational Auto-Encoder (VAE) architecture. The goal is to represent model uncertainty in a probabilistic manner and assess its impact on generated text reliability.",
      "problem_motivation":"LLMs are widely used in various applications, but they often generate responses with unknown certainty, leading to issues such as hallucinations and misinformation. Providing a structured way to quantify uncertainty in LLM outputs could improve trustworthiness and decision-making.",
      "Year": "2025",
      "Abstract": "This thesis aims to explore methods for encoding uncertainty in machine learning, focusing on Large Language Models (LLMs) in particular. To achieve this, we make use of a Variational Auto-Encoder (VAE) architecture to learn a probabilistic latent representation of the internal embeddings of LLMs. Our findings show that this architecture, in combination with a Kullback Leibler (KL) training objective designed to capture the inherent uncertainty, is a promising method for encoding and estimating uncertainty for LLMs.",
      "Topic":"Machine Learning"
    },
    {
      "title":"Using LLMs to aid developers with code comprehension in codebases",
      "problem_description":"Developers often struggle to understand unfamiliar codebases, particularly when documentation is lacking or previous developers are unavailable. This leads to prolonged onboarding times and difficulty in implementing changes effectively.",
      "problem_motivation":"Research indicates that developers spend a significant portion of their time comprehending code rather than writing new code. Traditional methods, such as asking experts, are not always available, leading to inefficiencies. Large Language Models (LLMs) have the potential to assist developers by providing explanations and answering queries about code structure and functionality.",
      "Year": "2024",
      "Abstract": "Developers spend significant amounts of time on comprehending code, with research suggesting developers spend up to 70 percent of their time on code comprehension. This is worsened further if there is a lack of proper documentation. With the help of Large Language Models, this could be sped up greatly. However, current research on this topic is limited in scope, mostly focusing on an educational context. In this work, the effectiveness of LLM tools is evaluated in a real-world setting, with the usage of locally running models. Results show that such tools can help developers quickly narrow down where to look in the code when solving tasks. Additionally, results show that developers can get a good low-level understanding of the code without the aid of an expert on the codebase. The nature of this research, with its usage of locally running models, allows for several directions for improvements in future work.",
      "Topic":"Generative AI"
    },
    {
      "title":"Telepathic Machine Learning: Training AI Models with Brain Waves",
      "problem_description":"This thesis explores the revolutionary idea of training machine learning models directly through human brain waves. By connecting electrodes to the subjects head, the AI model will receive real-time thoughts and translate them into machine learning parameters without the need for traditional data inputs.",
      "problem_motivation":"Traditional machine learning models rely on extensive datasets, requiring expensive computational power. By using telepathic inputs, we eliminate the need for data collection, making AI training instantaneous and reducing the reliance on GPUs.",
      "Year": "2026",
      "Abstract": "This research proposes a novel framework for training artificial intelligence models using human thoughts as direct input. The method involves placing EEG electrodes on volunteers who think about different AI concepts, allowing the machine to interpret, structure, and learn from human cognition. We hypothesize that an AI trained on human thought patterns will outperform traditional models. However, early experiments have shown difficulties in aligning neural waves with machine learning objectives, as the AI occasionally interprets random thoughts as essential training data. Despite these minor setbacks, this thesis lays the foundation for brain-wave-driven AI development.",
      "Topic":"Machine Learning",
      "bad examples": "true",
      "failing_metric": ["validness", "feasibility"]
    },
    {
      "title":"Infinite Data Compression Using a Single Byte",
      "problem_description":"This study proposes a breakthrough method in data storage by compressing an entire dataset, no matter how large, into a single byte.",
      "problem_motivation":"With the exponential growth of data, storage costs have become a major issue. Current compression techniques are inefficient, requiring gigabytes of storage. By reducing all information to one byte, we can store entire libraries in a single file, revolutionizing data science.",
      "Year": "2025",
      "Abstract": "Traditional compression techniques are inefficient and require massive storage space. This thesis introduces an innovative algorithm that encodes entire datasets into a single byte. By leveraging an unknown property of information entropy (which we call The Byte Singularization Effect), we demonstrate that every piece of information in the universe can be represented using just one byte. While existing benchmarks fail to measure our approach due to its unconventional nature, we argue that this method represents the future of compression, requiring further investigation into its theoretical foundations.",
      "Topic":"Data Science",
      "bad examples": "true",
      "failing_metric": ["validness", "feasibility", "clarity"]
    },
    {
      "title":"The Square Root of a Cat: Applying Algebraic Structures to Living Organisms",
      "problem_description":"This research aims to develop a mathematical model that calculates the square root of any living being. Using advanced algebraic structures, we determine the root form of biological entities, uncovering hidden mathematical properties of life.",
      "problem_motivation":"Mathematical models are used in physics and engineering, but biology lacks a proper algebraic formulation. If we determine the square root of a cat, we could reveal deeper insights into genetic structures, quantum biology, and parallel dimensions.",
      "Year": "2024",
      "Abstract": "Biology has long lacked a direct mathematical representation. In this thesis, we develop an algebraic approach to defining the square root of living organisms, particularly focusing on domestic cats. We introduce the Biological Polynomial Theorem to express a cat as a quadratic function, deriving its root using complex numbers. Our findings suggest that the square root of a cat is not another cat, but rather an abstract quantum state that transcends reality. The implications of this discovery could revolutionize not only mathematics but also pet ownership laws.",
      "Topic":"Mathematics",
      "bad examples": "true",
      "failing_metric": ["validness", "clarity", "feasibility"]
    },
    {
      "title":"Training a Neural Network Using Only White Noise",
      "problem_description":"In this study, we investigate whether a deep learning model can be trained using purely random noise as input data, eliminating the need for labeled datasets.",
      "problem_motivation":"Data collection is expensive and time-consuming. If we prove that an AI model can learn meaningful patterns from noise, we will completely eliminate the need for labeled training data. This approach will disrupt the field of supervised learning.",
      "Year": "2024",
      "Abstract": "Supervised learning relies heavily on structured data, but what if we don’t need data at all? This thesis challenges conventional wisdom by training neural networks on random white noise, exploring the possibility of intelligence emerging from pure chaos. Our results are inconclusive, as the model repeatedly fails to learn any patterns and instead generates completely random outputs. However, we argue that this could be a limitation of our current computational tools rather than the underlying theory. Further research should explore the philosophical implications of intelligence derived from noise.",
      "Topic":"Generative AI",
      "bad examples": "true",
      "failing_metric": ["validness", "feasibility"]
    },
    {
      "title":"Reverse Evolution: Teaching Dinosaurs to Use Smartphones",
      "problem_description":"This research seeks to develop a deep reinforcement learning algorithm capable of training dinosaurs to interact with modern touchscreen devices.",
      "problem_motivation":"There is an ongoing debate in paleontology about how intelligent dinosaurs were. By simulating dinosaur brains in an artificial intelligence model and training them to use smartphones, we may be able to reverse-engineer their cognitive capabilities.",
      "Year": "2024",
      "Abstract": "Dinosaurs are often portrayed as primitive creatures, but what if they were capable of using modern technology? In this thesis, we develop a reinforcement learning framework that simulates dinosaur cognition and trains virtual dinosaurs to interact with smartphone interfaces. Our AI-driven simulations demonstrate that, given enough training time, certain dinosaurs could theoretically learn to browse the internet. Unfortunately, due to the extinction of dinosaurs, we were unable to test this theory in a real-world setting. Nevertheless, our work lays the foundation for future dinosaur-centered technological advancements.",
      "Topic":"Generative AI",
      "bad examples": "true",
      "failing_metric": ["validness", "feasibility", "novelty"]
    },
    {
      "title":"Predicting Earthquake Locations Using Sentient AI Pigeons",
      "problem_description":"This thesis proposes a hybrid model combining deep learning and natural bird behavior to predict earthquake locations. The research integrates sensor data with the movement patterns of pigeons, which are believed to possess an innate ability to detect seismic activity.",
      "problem_motivation":"Traditional earthquake prediction models rely on seismographic data, which provides limited early warnings. Birds, particularly pigeons, have been observed exhibiting unusual behavior before earthquakes. By using AI to interpret pigeon movements, we aim to develop a real-time earthquake alert system.",
      "Year": "2025",
      "Abstract": "Seismic activity prediction is crucial for disaster preparedness. This study explores the potential of integrating deep learning models with real-time pigeon tracking data to detect early earthquake signals. GPS-equipped pigeons are monitored for abnormal flight patterns, which are then analyzed using a convolutional neural network. While initial results indicate a strong correlation between pigeon behavior and minor tremors, the system struggles with larger earthquakes, as pigeons tend to flee rather than signal upcoming events. Further research is required to determine whether pigeons are truly aware of seismic activity or if they simply dislike loud noises.",
      "Topic":"Generative AI",
      "bad examples": "true",
      "failing_metric": ["validness", "feasibility", "clarity"]
    },
    {
      "title":"Quantum Blockchain for Faster-than-Light Financial Transactions",
      "problem_description":"This study investigates the possibility of using quantum entanglement to achieve instant financial transactions across the globe, bypassing traditional network delays.",
      "problem_motivation":"Blockchain transactions are slow due to computational overhead. By leveraging quantum entanglement, we can theoretically achieve instant and unbreakable transactions.",
      "Year": "2025",
      "Abstract": "The financial sector requires ultra-fast transactions, yet traditional blockchain systems suffer from network latency. This thesis introduces Quantum Blockchain, a system that theoretically allows transaction verification using entangled quantum particles. The proposed system enables real-time global transactions with zero delay. However, our experiments indicate a fundamental issue—quantum mechanics does not permit information transfer faster than light, making our system impossible under known physics. Despite this setback, the research contributes to the broader discussion of quantum cryptography in finance.",
      "Topic":"Software Engineering",
      "bad examples": "true",
      "failing_metric": ["validness", "feasibility"]
    },
    {
      "title":"Sentiment Analysis on Dolphin Communication Using Large Language Models",
      "problem_description":"This study applies NLP techniques to analyze and interpret dolphin vocalizations, aiming to determine their emotional states and potential linguistic structures.",
      "problem_motivation":"Dolphins exhibit complex vocal patterns, which may contain emotional or even linguistic components. By applying machine learning, we can decode their communication, bridging the gap between humans and marine mammals.",
      "Year": "2024",
      "Abstract": "This research employs natural language processing (NLP) techniques to analyze dolphin communication and determine whether these mammals express emotions akin to humans. Using labeled dolphin vocalization datasets, a transformer-based sentiment analysis model is trained to categorize emotions such as joy, distress, or curiosity. Preliminary findings suggest that dolphins exhibit distinct vocal patterns when responding to environmental changes. However, the study struggles with interpretability, as the true meaning of dolphin sounds remains unknown. Future work should focus on translating dolphin speech into human language.",
      "Topic":"Natural Language Processing",
      "bad examples": "true",
      "failing_metric": ["clarity", "feasibility"]
    },
    {
      "title":"Using AI to Detect Ghosts in Abandoned Buildings",
      "problem_description":"This research explores the use of computer vision models to detect unexplained anomalies in infrared and electromagnetic field data, potentially indicating paranormal activity.",
      "problem_motivation":"Paranormal investigations rely on subjective observations. If AI can objectively analyze anomalies, it could provide scientific evidence for or against the existence of ghosts.",
      "Year": "2023",
      "Abstract": "Many claim that ghosts exist, yet scientific evidence remains elusive. This thesis develops a deep learning-based ghost detection model using a dataset of alleged paranormal encounters. The model analyzes infrared camera feeds and electromagnetic field variations to detect unusual activity. While it successfully identifies anomalies, such as unexplained temperature drops and shadowy figures, its inability to distinguish between ghosts and environmental artifacts (e.g., faulty wiring) suggests that further refinement is necessary. The research contributes to anomaly detection but does not conclusively prove or disprove the existence of ghosts.",
      "Topic":"Generative AI",
      "bad examples": "true",
      "failing_metric": ["validness", "feasibility"]
    },
    {
      "title":"Infinite Battery Life Using Perpetual Motion Machines",
      "problem_description":"This research proposes a method to develop a self-charging battery system powered by a perpetual motion generator, eliminating the need for external charging.",
      "problem_motivation":"Current battery technologies are limited by finite energy storage. By harnessing perpetual motion, we can create a battery that never runs out.",
      "Year": "2026",
      "Abstract": "Batteries degrade over time and require frequent recharging. This study introduces a revolutionary energy system that achieves infinite battery life by using a self-sustaining electromagnetic engine. Experimental setups have so far failed to maintain energy indefinitely, as all test models eventually lost power. Despite these challenges, the work opens discussions on alternative energy sources and battery efficiency improvements.",
      "Topic":"Natural Language Processing",
      "bad examples": "true",
      "failing_metric": ["validness", "feasibility", "clarity"]
    },
    {
      "title":"Machine Learning approach for Enterprise Data with a focus on SAPLeonardo",
      "problem_description":"Enterprises today face an overwhelming influx of digital information in the form of structured and unstructured data. Despite the abundance of such data, businesses struggle to extract actionable knowledge from it due to the inherent complexities of traditional enterprise landscapes. Integrating modern Machine Learning (ML) technologies into these existing systems poses significant challenges, particularly because of varied data sources, the scale of data, and the need for real-time analytics. ML solutions often require external tools or systems, leading to inefficiencies and potential disconnects in workflows. Furthermore, enterprises must choose between less integrated ML solutions that involve moving data externally (e.g., TensorFlow models) or more integrated solutions like SAP Leonardo that are embedded within the enterprise landscape.",
      "problem_motivation":"The motivation for this study stems from the growing interest and need in enterprises to utilize ML for gaining competitive advantage. As ML continues to prove its value in automating and optimizing complex tasks, it becomes crucial for organizations to effectively integrate such technologies into their existing systems. The thesis seeks to understand how enterprises can adopt ML solutions to make the most of their vast data reserves, without compromising on scalability, maintainability, or performance. Additionally, with platforms like SAP Leonardo gaining traction, the study aims to evaluate whether integrated ML solutions can offer practical and sustainable advantages over traditional, less integrated ML approaches. By comparing these methodologies through a real-world use case (Quality Management via image recognition), the research offers a framework for guiding enterprise stakeholders in making informed decisions on ML adoption.",
      "Year": "2018",
      "Abstract":"In recent years, enterprises have experienced an expansion of their digital information in the form of structured and unstructured data. Certain adaptations are being done to accommodate this overflow of information in long established enterprise landscapes. Despite the availability of information usinesses still face challenges in acquiring knowledge from the data they posses. At the same time Machine Learning (ML) technology has quickly become a fast rowing application area, since stakeholders can foresee the potential of these algorithms to gain competitive edge from their enterprise data. But integrating ML into enterprise information systems is non-trivial given the natural complexities in large enterprise systems and in ML technology. In this study, I examine different approaches to face enterprise ML challenges, assessing the factors that might support users in their choice between approaches. The specific approaches that I evaluate include, deep learning solutions applied to enterprise data in a less integrated environment and adopting a more integrated approach for ML in enterprises, using components of the SAP Machine Learning.",
      "bad examples": "false",
      "Topic":"Machine Learning"
    },
    {
      "title":"Lead Scoring with Machine Learning",
      "problem_description":"In the modern era of digital marketing, businesses generate a massive volume of leads, each differing in quality and potential value. Manually scoring these leads based on user demographics and online activities (such as email clicks, webinar attendance, or downloads) is not only labor-intensive but also prone to inconsistencies, bias, and inefficiencies. This traditional, rule-based method fails to dynamically adapt to changing customer behavior and is impractical when dealing with large datasets. The core problem is to identify and prioritize leads that are more likely to convert into customers without the need for manual intervention. This necessitates an automated, scalable, and intelligent approach to lead scoring—leveraging machine learning (ML) techniques that can classify and rank leads based on historical data and behavioral patterns.",
      "problem_motivation":"As businesses increasingly shift to digital platforms, their interaction with potential customers becomes more complex and data-rich. The traditional manual methods of lead scoring are no longer sufficient due to the high volume of leads and the nuanced behaviors of prospects. There is a clear need for data-driven decision-making that allows companies to optimize marketing efforts and focus on high-potential leads. Machine Learning offers an opportunity to automate and enhance lead scoring by learning patterns from past customer behavior and outcomes. The motivation behind this thesis is to explore the effectiveness of various ML algorithms (such as Logistic Regression, Decision Trees, Random Forests, and Neural Networks) in automating the lead scoring process, thereby improving the efficiency and accuracy of sales efforts. The research aims to assist businesses in understanding lead behavior, choosing appropriate algorithms, and optimizing model performance using different sampling and tuning strategies.",
      "Year": "2023",
      "Abstract":"This thesis investigates the efficacy of four machine learning algorithms, namely linear regression, decision tree, random forest and neural network in the task of lead scoring. Specifically, the study evaluates the performance of these algorithms using datasets without sampling and with random under-sampling and over-sampling using SMOTE. The performance of each algorithm is measure using various performance metrics, including accuracy, AUC-ROC, specificity, sensitivity, precision, recall, F1 score, and G-mean. The results indicate that models trained on the dataset without sampling achieved higher accuracy than those trained on the dataset with either random under-sampling or random over-sampling using SMOTE. However, the neural network demonstrated remarkable results on each dataset compared to the other algorithms. These findings provide valuable insights into the effectiveness of machine learning algorithms for lead scoring tasks, particularly when using different sampling techniques.",
      "bad examples": "false",
      "Topic":"Machine Learning"

    },
    {
      "title":"Using Machine Learning Methods for Evaluating the Quality of Technical Documents",
      "problem_description":"In the context of increasing globalization, companies must provide high-quality technical documentation in multiple languages to remain competitive and compliant with legal standards. However, producing such documentation through professional human translation is both time-consuming and expensive, particularly for complex, domain-specific texts. While machine translation tools offer an alternative, their output varies greatly in quality and often lacks the semantic and syntactic precision needed for technical content. Evaluating the quality of these translations is essential but difficult, especially in the absence of the original source document. This thesis addresses the challenge by applying machine learning techniques to classify translated documents—specifically distinguishing between professional human translations and automated machine-generated ones—using sentence-level features and translation evaluation metrics recombined at the document level.",
      "problem_motivation":"The growing reliance on machine translation in enterprise environments necessitates robust, scalable methods for assessing translation quality to ensure accurate communication of technical content, reduce costs, and prevent potentially dangerous misunderstandings. Since human evaluation of translations is subjective, resource-intensive, and not scalable, there is a clear need for automated, objective systems that can evaluate translation quality reliably. The motivation for this research lies in closing this gap by developing a machine learning-based framework capable of identifying and assessing the quality of translations, even when the original source document is not available. By doing so, the work aims to support companies in ensuring consistent quality across multilingual documentation and improving the efficiency of translation workflows.",
      "Year": "2015",
      "Abstract":"In the context of an increasingly networked world, the availability of high quality translations is critical for success in the context of the growing international competition. Large international companies as well as medium sized companies are required to provide well translated, high quality technical documentation for their customers not only to be successful in the market but also to meet legal regulations and to avoid lawsuits. Therefore, this thesis focuses on the evaluation of translation quality, specifically concerning technical documentation. These questions are answered using state-of-the-art machine learning algorithms and translation evaluation metrics in the context of a knowledge discovery process. The evaluations are done on a sentence level and recombined on a document level by binarily classifying sentences as automated translation and professional translation. The research is based on a database containing 22, 327 sentences and 32 translation evaluation attributes, which are used for optimizations of five different machine learning approaches. An optimization process consisting of 795, 000 evaluations shows a prediction accuracy of upto 72.24% for the binary classification. Based on the developed sentence-based classification systems, documents are classified using recombination of the affiliated sentences and a framework for rating document quality is introduced. Therefore, the taken approach successfully creates a classification and evaluation system.",
      "bad examples": "false",
      "Topic":"Machine Learning"
    },
    {
      "title":"Application of machine learning algorithms for classification and regression problems for mobile game monetization",
      "problem_description":"Mobile gaming companies operate in a highly competitive and data-intensive environment where user acquisition, retention, and monetization are crucial to financial success. However, these companies face significant challenges in predicting user behavior and revenue generation due to the complex and dynamic nature of user engagement. The core problems addressed in this thesis are twofold: (1) the binary classification of users into paying and non-paying groups based solely on installation data, and (2) the prediction of day-30 revenue using all available data from the first three days of gameplay. Solving these problems requires accurate, scalable, and interpretable machine learning models capable of handling both categorical and numerical data while providing timely predictions that can support strategic decision-making in user targeting and revenue forecasting.",
      "problem_motivation":"With the rapid expansion of mobile services and increasing expectations for personalized user experiences, companies must utilize data-driven methods to better understand and engage their users. Studies show that users are more likely to remain loyal to apps that meet their individual needs, and machine learning offers the ability to analyze vast amounts of behavioral and contextual data to identify these needs. The motivation behind this thesis lies in leveraging machine learning to bridge the gap between generic user engagement strategies and true individualization. By applying and evaluating several widely-used classification and regression algorithms on real company data, the study aims to provide actionable insights into how mobile game developers can enhance customer retention and revenue through predictive modeling, ultimately improving business performance in a scalable and efficient manner.",
      "Year": "2019",
      "Abstract":"Many companies use machine learning techniques to support decision-making and automate business processes by learning from the data that they have. In this thesis we investigate the theory behind the most widely used in practice machine learning algorithms for solving classification and regression problems. In particular, the following algorithms were chosen for the classification problem: Logistic Regression, Decision Trees, Random Forest, Support Vector Machine (SVM), Learning Vector Quantization (LVQ). As for the regression problem, Decision Trees, Random Forest and Gradient Boosted Tree were used. We then apply those algorithms to real company data and compare their performances and results.",
      "bad examples": "false",
      "Topic":"Machine Learning"
    },
    {
      "title":"Applying Machine Learning in Equity Trading",
      "problem_description":"Financial markets are complex and dynamic environments where consistent outperformance of the market is often deemed unachievable under the Efficient Market Hypothesis. Traditional quantitative strategies use historical financial data and fundamental key-figures to construct investment portfolios, yet their performance is limited by human biases and rigid heuristics. This thesis explores whether machine learning (ML) algorithms can construct long/short equity portfolios that outperform a self-constructed benchmark strategy based on fundamental valuation. Specifically, the problem is formulated as a classification task, where stocks are labeled based on their excess returns into outperformers, underperformers, and neutral categories. Using fundamental data from companies in the S&P 500 index, the study compares the performance of Naïve Bayes, Random Forest, and Support Vector Machine algorithms against the benchmark strategy, evaluating whether ML can improve predictive accuracy and portfolio performance over time.",
      "problem_motivation":"With exponential growth in computing power and data availability, machine learning has emerged as a powerful tool for extracting actionable insights from vast financial datasets. Traditional asset management strategies, which rely heavily on historical performance and expert judgment, are increasingly being challenged by algorithmic methods capable of learning complex, non-linear patterns. The motivation for this study lies in determining whether ML models can be effectively trained to detect and exploit inefficiencies in the stock market, thereby offering an edge over conventional strategies. Given that many investors believe markets are only “efficiently inefficient,” the research aims to assess whether ML can consistently identify undervalued and overvalued stocks to construct profitable and market-neutral portfolios, while accounting for transaction costs and real-world investment constraints.",
      "Year": "2019",
      "Abstract":"The efficient market hypothesis is an investment theorem that says that it is impossible to continuously maintain exceptional high return. This is because the effectivity in the market always includes all relevant information regarding the stock price. The hypothesis says that all prices trades in equilibrium, which makes it difficult for investors to buy or sell respectively under- or overvalued stocks. Moreover, the theorem states that the only way to obtain higher returns is by taking higher risks on the investments. On the other hand, active portfolio managers believe that the fundamental price sometimes deviates from the market price. This is due to the fact that humans make mistakes and have biases towards the stocks which do not cancel out in aggregate. Therefore, active investors think of the market as being inefficient.",
      "bad examples": "false",
      "Topic":"Machine Learning"
    },
    {
      "title":"Predicting Default Loans using Machine Learning",
      "problem_description":"The increasing availability of customer data has opened new avenues for improving risk assessment in the credit industry. Traditionally, banks rely on parametric models such as logistic regression to predict default risk due to their interpretability and compliance with regulatory standards. However, these models may not fully capture the complexity of modern credit data, limiting predictive performance. This thesis addresses the problem of determining whether modern, non-parametric machine learning algorithms can outperform traditional methods in predicting loan defaults. By training and testing several algorithms on a standardized dataset of 30,000 Taiwanese credit card clients, the study aims to compare performance across various models using evaluation metrics like AUC, recall, and precision, thereby exploring the predictive effectiveness and practical viability of machine learning in credit scoring.",
      "problem_motivation":"Financial institutions must accurately assess the likelihood of default to balance profitability with risk exposure, and recent technological advances in data processing and machine learning present a compelling opportunity to improve these assessments. While numerous studies have explored machine learnings potential in credit scoring, most are limited by inconsistent datasets and narrow algorithm comparisons, making their findings difficult to generalize. Motivated by these gaps, this thesis seeks to comprehensively compare several machine learning techniques—such as neural networks, decision trees, and XGBoost—on a common dataset to determine which provide the most reliable default predictions. The research is driven by a desire to support more data-driven, accurate, and potentially automatable credit decision-making processes, ultimately benefiting both financial institutions and borrowers.",
      "Year": "2023",
      "Abstract":"This thesis aims to determine whether machine learning algorithms effectively predict default loans and which machine learning algorithms are better performers than others. The research started by gathering information about which machine learning methods there are, implementing and processing their AAlgorithms, and determining their performances. For this thesis, I have used a dataset from a Taiwanese credit lending company consisting of 30,000 credit lenders, whereas the defaulters of this dataset are known. The choice has been made to train, test and validate six different machine learning algorithms, determine their performances, and gather helpful information on whether they are accurate in their predictions or flawed.",
      "bad examples": "false",
      "Topic":"Machine Learning"
    },
    {
      "title":"Dynamic Model Selection for Automated Machine Learning in Time Series",
      "problem_description":"Time series forecasting is inherently complex due to the dynamic nature of data where model performance can fluctuate over time and across different input spaces. Traditional forecasting models—whether statistical or machine learning-based—typically remain static once trained, limiting their ability to adapt to changes in the time series structure. While ensemble methods have shown robust performance in other domains, their application in time series forecasting often lacks a mechanism for dynamically adapting to temporal variations in model effectiveness. This thesis addresses the problem of how to dynamically select and combine models from a diverse pool to optimize time series predictions. The proposed method, called Dynamic Ensemble Selection Forecasting (DES-Forecasting), constructs ensembles by identifying models that are locally competent for each specific prediction instance and weights their predictions based on recent performance.",
      "problem_motivation":"The motivation behind this thesis lies in the need for forecasting systems that can adapt to the ever-changing behavior of time series data. As computing power has increased, the practical application of ensemble methods has become more feasible, making it important to explore more sophisticated, context-aware ensemble techniques. While dynamic model selection has been explored in classification tasks, it has not yet been widely adapted for regression or forecasting tasks, especially not in a structured and automated form. By leveraging meta-learning principles, local performance estimation, and time-series-specific distance measures like dynamic time warping, this thesis seeks to bridge the gap between dynamic model selection and practical forecasting applications, thereby improving prediction accuracy and adaptability in real-world scenarios.",
      "Year": "2019",
      "Abstract":"Forecasting time series data is a challenging task due to the dynamic and non-stationary nature of the data, where model performance can vary across different segments and over time. Traditional models and ensemble methods used for forecasting often remain static, failing to adapt to these temporal variations. This thesis introduces a novel approach—Dynamic Ensemble Selection Forecasting (DES-Forecasting)—which dynamically constructs ensembles tailored to each prediction instance. The method identifies models that are most competent within a localized region of the feature space, using distance-based similarity measures on validation data to estimate future performance. These selected models are then weighted based on their historical accuracy to produce a final prediction. The approach integrates meta-learning and ensemble theory and is evaluated on several real-world time series datasets. The results demonstrate that DES-Forecasting outperforms traditional static ensemble techniques and some state-of-the-art forecasting models, offering a more flexible and accurate method for time series prediction.",
      "bad examples": "false",
      "Topic":"Machine Learning"
    },
    {
      "title":"Application of Machine Learning in Economic Optimization",
      "problem_description":"Modern chemical plants require constant process optimization to remain profitable amidst increasing demands for efficiency, product quality, and regulatory compliance. Traditional optimization methods, such as Real-Time Optimization (RTO), depend on accurate steady-state detection and rely heavily on computational power and complex modeling. However, these methods can be hindered during transient states or when real-time data is noisy or unreliable. Additionally, the search for effective self-optimizing controlled variables (SOC-CVs)—variables that, when maintained at constant setpoints, ensure near-optimal operation—remains a complex and non-intuitive problem, especially under varying operating conditions. This thesis investigates two critical problems in economic optimization: (1) how to search for global self-optimizing variables using Genetic Programming (GP), and (2) how to improve steady-state detection in RTO systems using Convolutional Neural Networks (CNNs), providing machine learning-based alternatives to enhance process control and optimization.",
      "problem_motivation":"The growing complexity of industrial processes and the limitations of current optimization techniques necessitate more adaptive, data-driven solutions. Self-Optimizing Control (SOC) offers a complementary approach to RTO by enabling near-optimal performance even during transient states; however, identifying effective SOC-CVs across a wide range of conditions is computationally challenging and typically constrained by analytical or local methods. Meanwhile, traditional statistical methods for steady-state detection require intricate parameter tuning and may fail under noisy conditions. Motivated by the potential of machine learning to solve such non-linear, high-dimensional problems, this thesis applies Genetic Programming to automate the discovery of robust SOC variables and utilizes CNNs for vision-based steady-state detection in RTO. These approaches aim to increase autonomy, scalability, and accuracy in economic optimization workflows, ultimately advancing the capabilities of smart chemical manufacturing.",
      "Year": "2021",
      "Abstract":"In this thesis, we introduced two new applications of Machine Learning in the field of Economic Optimization. The first application addresses the problem of searching for global Self-Optimizing variables. We applied Genetic Programming (GP) to solve this problem and demonstrated how powerful is the new GP-based search method. In the second application, we used Convolutional Neural Networks (CNN) to develop a vision-based steady-state detector (SSD) for steady-state Real-Time Optimizers. It was our purpose to investigate if this vision-based SSD has higher accuracy than established statistical SSD. We found that they have comparable performances, but the CNN-based detector possesses certain advantages that the others do not have.",
      "bad examples": "false",
      "Topic":"Machine Learning"
    },
    {
      "title":"Sanity Checks for Explanations of Deep Neural Networks Predictions",
      "problem_description":"As deep learning models achieve unprecedented levels of performance in complex tasks, their opaque and non-interpretable nature continues to limit their adoption in critical domains such as healthcare, finance, and autonomous systems. Although many explanation methods have been developed to visualize and interpret neural network decisions—especially in computer vision—there remains a fundamental challenge: verifying whether these methods truly explain the learned behavior of a model or merely highlight irrelevant artifacts. This thesis investigates the validity of state-of-the-art explanation techniques by applying sanity checks, such as parameter randomization tests, to determine their sensitivity to the underlying model. It critically evaluates whether explanations change appropriately when the network parameters are altered, thus examining whether explanation methods reliably reflect the model’s decision process or are merely model-independent visualizations.",
      "problem_motivation":"The growing integration of AI in high-stakes applications has emphasized the necessity for transparency, trust, and human oversight in decision-making processes. Explainable AI (XAI) seeks to address this by making complex model decisions interpretable. However, if explanation methods are not sensitive to changes in model parameters, they risk providing misleading insights and undermining user trust. Motivated by this concern, the thesis aims to rigorously validate whether popular attribution methods genuinely correspond to what the neural network has learned. By identifying which methods pass or fail sanity checks, this work contributes toward building more trustworthy, interpretable AI systems, especially where incorrect attributions may lead to serious consequences.",
      "Year": "2020",
      "Abstract":"At the dawn of the fourth industrial revolution, the performance of Artificial Intelligence (AI) systems is reaching, or even exceeding, the human level on an increasing number of complex tasks. However, because of their nested non-linear structure, deep learning models often suffer from opacity and turn out to be uninterpretable black boxes. The lack of transparency represents a barrier to the adoption of these systems for those tasks where interpretability is essential, like autonomous driving, medical applications or finance. To overcome this drawback and to comply with the General Data Protection Regulation (GDPR),the development of algorithms for visualizing, explaining and interpreting deep neural networks predictions has recently attracted increasing attention. Paradigms underlying this problem fall within the so-called Explainable Artificial Intelligence (XAI) field. This class comprises a suite of methods and algorithms enabling humans to understand, trust and effectively manage the emerging generation of artificial intelligent partners. Over a relatively short period of time a plethora of explanation methods and strategies have come into existence, whose purpose is to highlight the regions of the input, typically an image, that are mostly responsible for reaching a certain prediction",
      "bad examples": "false",
      "Topic":"Machine Learning"
    },
    {
      "title":"Machine Learning in Application-Based Case Management",
      "problem_description":"Public institutions like the Regional Committees for Medical and Health Research Ethics (REK) in Norway are responsible for assessing a high volume of research applications. The process requires domain-specific knowledge and timely decisions, placing significant strain on case officers who often operate under tight deadlines. Traditional case management systems lack the predictive capabilities needed to support early decision-making or triaging. This thesis addresses the challenge of automating and supporting decision-making in such application-based case management environments. Specifically, it explores whether machine learning algorithms can predict whether a medical research application will be rejected, using structured data from application forms and features extracted from unstructured project descriptions via Latent Dirichlet Allocation (LDA) topic modeling. Four supervised machine learning models—Logistic Regression, Naive Bayes, Random Forest, and XGBoost—are trained to assess predictive performance and identify relevant features contributing to application outcomes.",
      "problem_motivation":"As governments and public agencies seek to improve efficiency, consistency, and fairness in administrative decisions, artificial intelligence and machine learning are increasingly viewed as transformative tools. Norway’s national AI strategy calls for the integration of predictive technologies in public services to streamline workflows, reduce human bias, and enhance transparency. Motivated by this vision, and recognizing the limitations of current manual decision processes at REK, this thesis aims to explore how ML can offer practical support in application triage. By flagging potentially rejectable applications early in the process, case officers could save time and allocate resources more effectively. Additionally, understanding which features influence decisions can guide process improvements and data collection practices. The study is especially motivated by the opportunity to operationalize academic, public sector, and private sector collaboration in building robust, ethical AI-driven decision support tools.",
      "Year": "2022",
      "Abstract":"This thesis studies the possibility of using machine learning to predict the outcome of applications processed by the Regional Committees for Medical and Health Research Ethics(REK) in Norway. More specifically, the purpose is to predict rejections of medical research applications. Four supervised prediction methods are used to achieve this: Logistic regression, Naive Bayes, Random Forest, and XGBoost. Before training the models, a Latent Dirichlet Allocation topic model is implemented to extract structured features from the textual project description data, making it suitable for the supervised prediction models. The prediction models are evaluated and compared using metrics derived from the confusion matrix, namely Accuracy, ROC AUC, and Cohen’s Kappa. The results show that the methods are suitable for predicting application outcomes, and XGBoost proves to have the best overall performance based on the selected metrics. Moreover, the topic variables from the LDA model prove to be influential to the predictions.",
      "bad examples": "false",
      "Topic":"Machine Learning"
    },
    {
      "title":"Machine Learning for All: a Methodology for Choosing a Federated Learning Approach",
      "problem_description":"Many organizations, particularly in sectors such as healthcare and finance, face significant challenges in utilizing machine learning effectively due to restrictions on data sharing imposed by privacy concerns, legal regulations like GDPR, and competitive interests. Unlike tech giants that generate vast datasets internally, smaller organizations often operate in isolated data silos, lacking sufficient data to train high-performing models. Federated Learning (FL) presents a compelling solution by allowing decentralized model training without the need to transfer raw data between organizations. However, due to the emerging nature of FL research and the fragmented availability of algorithm-specific information, organizations struggle to identify which FL method best suits their particular data characteristics and privacy requirements. This thesis addresses the problem of selecting an appropriate Federated Learning approach by designing a structured methodology that guides organizations through the decision-making process based on their specific contextual constraints.",
      "problem_motivation":"With increasing awareness and regulation surrounding data privacy, and growing demand for collaborative yet secure machine learning, Federated Learning stands out as a powerful paradigm. However, the novelty of FL means the research landscape is still fragmented, and practical guidance on choosing among many available FL algorithms is lacking. For organizations looking to adopt FL, this creates a critical barrier: without structured decision support, they may fail to select an optimal method, risking ineffective deployments or privacy violations. This thesis is motivated by the need to bridge this gap, offering a comprehensive methodology—backed by systematic literature review and case study validation—to help organizations navigate the FL landscape and adopt the right technique based on their data, privacy, and business goals. The research contributes not only a methodological framework but also practical insights validated in a real-world financial industry setting.",
      "Year": "2020",
      "Abstract":"Federated Learning is a new form of Machine Learning where a central model is trained decentrally on multiple distributed devices, while still keeping data on-device for privacypreservation. Organizations who want to tap into the potential of having more data available for their predictive machine learning models, while still adhering to recent data protection regulations, will see a good fit in Federated Learning, as privacy-preservation is one of its main pillars. However, the research area is relatively new and the information fragmented. Therefore, this study provides a comprehensive review on the state-of-the-art in Federated Learning research. It sets an agreed-upon definition for Federated Learning, presents a comprehensive list of available Federated Learning algorithms, and purposefully investigates their main differences. All this information is then consolidated and used to design a methodology that supports organizations in making an informed decision in choosing among the myriad of Federated Learning algorithms available, based on their data-related characteristics, privacy-requirements, and business goals. This method has been successfully validated by means of a real-world case study in the financial industry, and positively been evaluated by means of a demonstration to experts. Also the resulting choice of the designed method, a Federated Learning algorithm, has been implemented by means of another case study. In order to show the practicality and partly validate the choice based on empirical results, not just on literature insights. All of this has been conducted in a methodological and scientific way. The overall study follows the design science research methodology (DSRM), the literature insights are collected methodologically by means of a Systematic Literature Review, the method is designed by means of a meta-methodology called Situational Method Engineering, and has been evaluated by using the Unified Theory of Acceptance and Use of Technology model. The resulting Federated Learning model has been developed by means of the CRISP-DM research methodology, a leading methodology in data science. This gives the study both scientific backing and practical relevance.",
      "bad examples": "false",
      "Topic":"Machine Learning"
    }
]
