[
  {
    "title": "User-Centric Evaluation Metrics for LLM-Based Natural Language Interfaces",
    "description": "Existing evaluation metrics for LLMs often rely on automated benchmarks that do not fully capture user experiences. This research proposes new, user-centric metrics to assess the usability and effectiveness of LLM-based natural language interfaces (NLIs).",
    "motivation": "Automated evaluations may miss critical aspects of user satisfaction and task performance. Developing user-centric metrics can provide a more holistic view of LLM usability, ensuring that these models meet real-world needs. This idea is grounded in the limitations highlighted by 'LLM Comparator: Interactive Analysis of Side-by-Side Evaluation of Large Language Models' which emphasizes the need for scalable and interpretable evaluation methods.",
    "questions": [
      "How do user satisfaction metrics such as perceived ease of use and usefulness correlate with traditional automated benchmarks for LLM-based natural language interfaces?",
      "What are the most effective methods to quantify user engagement and retention in evaluating LLM-based NLIs, and how do they compare to current evaluation techniques?",
      "In what ways do demographic factors (such as age, educational background, and technological proficiency) influence users' perceptions of usability and effectiveness when interacting with LLM-based natural language interfaces?"
    ]
  }
]