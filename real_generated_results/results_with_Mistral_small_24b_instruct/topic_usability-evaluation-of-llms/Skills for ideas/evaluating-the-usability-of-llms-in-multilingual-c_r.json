[
  {
    "title": "Evaluating the Usability of LLMs in Multilingual Contexts",
    "description": "Current usability evaluations of Large Language Models (LLMs) primarily focus on English-speaking users, neglecting the diverse linguistic needs and cultural nuances of non-English speakers. This research aims to develop a comprehensive framework for evaluating the usability of LLMs across multiple languages.",
    "motivation": "The global reach of AI applications necessitates that LLMs be usable by speakers of various languages. Existing evaluations often overlook the unique challenges faced by non-English users, such as language-specific idioms and cultural references. This gap can lead to suboptimal user experiences and reduced effectiveness in multilingual contexts.",
    "skills": [
      "Python",
      "NLP libraries (e.g., spaCy, NLTK)",
      "Multilingual models (e.g., mBERT, XLM-R)",
      "Usability evaluation methods",
      "User experience design",
      "Statistical analysis",
      "Natural language processing",
      "Machine learning frameworks (e.g., TensorFlow, PyTorch)",
      "Cross-lingual alignment techniques",
      "Data collection and annotation tools"
    ]
  }
]